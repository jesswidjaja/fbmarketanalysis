{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import json\n",
    "import datetime\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_id = "[your APP ID]",
    "app_secret = "[your SECRET ID]",
    "page_id = "[the page you want to scrape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = app_id + \"|\" + app_secret\n",
    "access_token = "[your ACCESS TOKEN from Graph API explorer]",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "limited = True \n",
    "limit = 100 #number of pages to be processed if limit is true\n",
    "\n",
    "delimiter = \";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "def request_until_succeed(url):\n",
    "    req = urllib.request.Request(url)\n",
    "    success = False\n",
    "    while success is False:\n",
    "        try:\n",
    "            response = urllib.request.urlopen(req)\n",
    "            if response.getcode() == 200:\n",
    "                success = True\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(5)\n",
    "            \n",
    "            print(\"Error for URL %s: %s\" % (url, datetime.datetime.now()))\n",
    "            \n",
    "    return response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_normalize(text):\n",
    "    return text.translate({ 0x2018:0x27, 0x2019:0x27, 0x201C:0x22, 0x201D:0x22, 0xa0:0x20 }).encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFacebookPageFeedData(page_id, access_token, num_statuses):\n",
    "    #create URL string\n",
    "    base = \"https://graph.facebook.com/v2.10/\" #version changes\n",
    "    node = \"/\" + page_id + \"/posts\"\n",
    "    parameters = \"/?fields=message,link,created_time,type,name,id,comments.limit(1).summary(true),shares,reactions.limit(1).summary(true)&limit=%s&access_token=%s\" % (num_statuses, access_token) # changed\"\n",
    "    url = base + node + parameters\n",
    "    \n",
    "    #get data\n",
    "    data = json.loads(request_until_succeed(url))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFacebookPostData(page_id, access_token):\n",
    "    num_statuses = 1\n",
    "    #create URL string\n",
    "    base = \"https://graph.facebook.com/v2.10/\"\n",
    "    node = page_id\n",
    "    parameters = \"/?fields=reactions.summary(total_count),reactions.type(LIKE).limit(0).summary(total_count).as(like),reactions.type(LOVE).limit(0).summary(total_count).as(love),reactions.type(WOW).limit(0).summary(total_count).as(wow),reactions.type(HAHA).limit(0).summary(total_count).as(haha),reactions.type(SAD).limit(0).summary(total_count).as(sad),reactions.type(ANGRY).limit(0).summary(total_count).as(angry),reactions.type(THANKFUL).limit(0).summary(total_count).as(thankful)&limit=%s&access_token=%s\" % (num_statuses, access_token) # changed\"\n",
    "    url = base + node + parameters\n",
    "    \n",
    "    #get data\n",
    "    data = json.loads(request_until_succeed(url))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processFacebookPageFeedStatus(status):\n",
    "    status_id = status['id']\n",
    "    data = getFacebookPostData(status_id,access_token)\n",
    "\n",
    "    \n",
    "    status_message = '' if 'message' not in status.keys() else unicode_normalize(status['message'])\n",
    "    link_name = '' if 'name' not in status.keys() else unicode_normalize(status['name'])\n",
    "    status_type = status['type']\n",
    "    status_link = '' if 'link' not in status.keys() else unicode_normalize(status['link'])\n",
    "    \n",
    "    status_published = datetime.datetime.strptime(status['created_time'],'%Y-%m-%dT%H:%M:%S+0000')\n",
    "    status_published = status_published + datetime.timedelta(hours=-5) # EST\n",
    "    status_published = status_published.strftime('%Y-%m-%d %H:%M:%S') # best time format for spreadsheet programs\n",
    "    \n",
    "    #num_reactions1 = 0 if 'reactions' not in status.keys() else status['reactions']['type']['like']#.limit(0).summary(true).as(like)\n",
    "    num_reactions = 0 if 'reactions' not in status.keys() else status['reactions']['summary']['total_count']\n",
    "    num_comments = 0 if 'comments' not in status.keys() else status['comments']['summary']['total_count']\n",
    "    num_shares = 0 if 'shares' not in status.keys() else status['shares']['count']\n",
    "    num_likes = 0 if 'like' not in data.keys() else data['like']['summary']['total_count']\n",
    "    num_loves = 0 if 'love' not in data.keys() else data['love']['summary']['total_count']\n",
    "    num_wows = 0 if 'wow' not in data.keys() else data['wow']['summary']['total_count']\n",
    "    num_hahas = 0 if 'haha' not in data.keys() else data['haha']['summary']['total_count']\n",
    "    num_sads = 0 if 'sad' not in data.keys() else data['sad']['summary']['total_count']\n",
    "    num_angries = 0 if 'angry' not in data.keys() else data['angry']['summary']['total_count']\n",
    "    num_thankfuls = 0 if 'thankful' not in data.keys() else data['thankful']['summary']['total_count']\n",
    "    # return a tuple of all processed data\n",
    "    \n",
    "    return (status_id, status_message, link_name, status_type, status_link,\n",
    "           status_published, num_reactions, num_comments, num_shares, num_likes,\n",
    "            num_loves, num_wows, num_hahas, num_sads, num_angries, num_thankfuls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeFacebookPageFeedStatus(page_id, access_token):\n",
    "    with open('%s_facebook_statuses.csv' % page_id, 'w') as file:\n",
    "        w = csv.writer(file, delimiter = delimiter)\n",
    "        w.writerow([\"status_id\", \"status_message\", \"link_name\", \"status_type\", \"status_link\",\n",
    "           \"status_published\", \"num_reactions\", \"num_comments\", \"num_shares\", \"num_likes\",\n",
    "                    \"num_loves\",\"num_wows\",\"num_hahas\",\"num_sads\",\"num_angries\",\"num_thankfuls\"])\n",
    "        \n",
    "        has_next_page = True\n",
    "        num_processed = 0   # keep a count on how many we've processed\n",
    "        scrape_starttime = datetime.datetime.now()\n",
    "        \n",
    "        print(\"Scraping %s Facebook Page: %s\\n\" % (page_id, scrape_starttime))\n",
    "        \n",
    "        statuses = getFacebookPageFeedData(page_id, access_token, 100)\n",
    "        #print(statuses)\n",
    "        \n",
    "        while has_next_page:\n",
    "            for status in statuses['data']:\n",
    "                w.writerow(processFacebookPageFeedStatus(status))\n",
    "                \n",
    "                # output progress occasionally to make sure code is not stalling\n",
    "                num_processed += 1\n",
    "                if num_processed % 100 == 0:\n",
    "                    print(\"%s Statuses Processed: %s\" % (num_processed, datetime.datetime.now()))\n",
    "                if limited == True and num_processed >= limit:\n",
    "                    has_next_page = False\n",
    "                    \n",
    "            # if there is no next page, we're done.\n",
    "            if 'paging' in statuses.keys():\n",
    "                statuses = json.loads(request_until_succeed(statuses['paging']['next']))\n",
    "            else:\n",
    "                has_next_page = False\n",
    "                \n",
    "        \n",
    "        print(\"\\nDone!\\n%s Statuses Processed in %s\" % (num_processed, datetime.datetime.now() - scrape_starttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping ucsbhacks Facebook Page: 2017-11-01 20:29:55.517586\n",
      "\n",
      "100 Statuses Processed: 2017-11-01 20:30:32.063625\n",
      "\n",
      "Done!\n",
      "100 Statuses Processed in 0:00:38.321125\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\tscrapeFacebookPageFeedStatus(page_id, access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
